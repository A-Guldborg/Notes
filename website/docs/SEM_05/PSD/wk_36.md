Material: Chapter 3 up to and including 3.6.3

# (2) Lexers

Using tools `fxlex` and `fsyacc`.

Lexers aim to convert input data into tokens that represent the data as objects in program code. This can then be used in a parser to determine what the tokens are used for.

![From text input to abstract syntax](lexer_and_parser.png)

## Lexer specification (Syntax)

Written in `.fsl`-files and uses regular expressions.

If there are any auxiliary functions etc, they are at the beginning of the lexer specification file and enclosed in curly braces {}. Best practice is for instance to have reserved keywords and variable names as an auxiliary function, such that all "legal" variable-names are sent through the auxiliary function `keyword s`. The `keyword` function attempts to match the input `s` with an existing keyword and if none is found, returns a `NAME s` token. 

After the helper functions, the lexer specification has the tokenizer `Token` with regexes as below to describe how to parse inputs and which tokens to generate accordingly. 

```fs
rule Token = parse
  | [’ ’ ’\t’ ’\n’ ’\r’]                        { Token lexbuf }
  | [’0’-’9’]+                                  { CSTINT (...) }
  | [’a’-’z’’A’-’Z’][’a’-’z’’A’-’Z’’0’-’9’]*    { keyword (...) }
  | ’+’                                         { PLUS }
  | ’-’                                         { MINUS }
  | ’*’                                         { TIMES }
  | ’(’                                         { LPAR }
  | ’)’                                         { RPAR }
  ...
  | eof                                         { EOF }
  | _                                           { lexerError lexbuf "Bad char" }
```

Left-side is regular expressions that matches the textual input, whereas the right side is the tokens that is used by the program.

To turn the lexer specification into a lexer program, we run the following command, where `ExprLex.fsl` is the lexer specification file:
```sh
fslex --unicode ExprLex.fsl
```

## Parsers

A parser accepts a stream of tokens from a lexer and builds an abstract syntax tree. The parser is responsible for precedence rules (such as * before +).

Precedence is defined in the `.fsy` files. For example:
```fsyacc
%left MINUS PLUS        /* lowest precedence  */
%left TIMES             /* highest precedence */
```

In this example, we see that MINUS and PLUS binds left, i.e. 1-2+3 makes (1-2)+3.
We also see that `TIMES` is defined after the first two, meaning it has higher precedence. In other words, 1-2*3 becomes 1-(2*3).

To generate the Parser program, run the following command, where `ExprPar` is the resulting module definition and `ExprPar.fsy` is the parser specification file:
```sh
fsyacc --module ExprPar ExprPar.fsy
```

## Lexer and parser generators

`fslex` is a lexer generator and `fsyacc` is a parser generator, turning specifications files into the according lexer and parser programs.

A full example of using `fslex` and `fsyacc` is the below:

```sh
fslex --unicode ExprLex.fsl
fsyacc --module ExprPar ExprPar.fsy
fsharpi -r FSharp.PowerPack.dll Absyn.fs ExprPar.fs ExprLex.fs Parse.fs
```

The first two are described above, the third loads the abstract syntax (`Absyn.fs`) and the resulting parser and lexer into the interactive fsharp environment in the command line. It also includes the `Parse.fs` which is a file that allows us to read input from strings or files, inputs such as "3 * 2 + 1".

The `Parse.fs` looks like this:

```fs
(* File Expr/Parse.fs *)
(* Lexing and parsing of simple expressions using fslex and fsyacc *)

module Parse

open System
open System.IO
open System.Text
open (*Microsoft.*)FSharp.Text
open Absyn

(* Plain parsing from a string, with poor error reporting *)

let fromString (str : string) : expr =
    let lexbuf = Lexing.LexBuffer<char>.FromString(str)
    try 
      ExprPar.Main ExprLex.Token lexbuf
    with 
      | exn -> let pos = lexbuf.EndPos 
               failwithf "%s near line %d, column %d\n" 
                  (exn.Message) (pos.Line+1) pos.Column
             
(* Parsing from a text file *)

let fromFile (filename : string) =
    use reader = new StreamReader(filename)
    let lexbuf = Lexing.LexBuffer<char>.FromTextReader reader
    try 
      ExprPar.Main ExprLex.Token lexbuf
    with 
      | exn -> let pos = lexbuf.EndPos 
               failwithf "%s in file %s near line %d, column %d\n" 
                  (exn.Message) filename (pos.Line+1) pos.Column
```